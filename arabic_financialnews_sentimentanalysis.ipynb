{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "import joblib\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "uHwMpiWZFq_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "RmTRv0_QCZYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_input = 'financial_phrasebank_arabic.csv'\n",
        "df = pd.read_csv(path_input, lineterminator='\\n')\n",
        "\n",
        "# Remove the extra sentence from each row\n",
        "df['sentence'] = df['sentence'].apply(lambda x: x.split('ترجمة')[0] if 'ترجمة' in x else x)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qrsEWxhPHB4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "train.to_csv(\"train.csv\",index=False)\n",
        "val.to_csv(\"val.csv\",index=False)"
      ],
      "metadata": {
        "id": "nI8sQ9-JHnWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicDataset(Dataset):\n",
        "    def __init__(self,data,max_len):\n",
        "        super().__init__()\n",
        "        self.labels = data[\"label\"].values\n",
        "        self.texts = data[\"sentence\"].values\n",
        "        self.max_len = max_len\n",
        "        model = 'aubmindlab/bert-base-arabertv2'\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        text = \" \".join(self.texts[idx].split())\n",
        "        label = self.labels[idx]\n",
        "        inputs = self.tokenizer(text,padding='max_length',\n",
        "                                max_length=self.max_len,truncation=True,return_tensors=\"pt\")\n",
        "        #input_ids,token_type_ids,attention_mask\n",
        "        return {\n",
        "            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n",
        "                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n",
        "                      \"attention_mask\":inputs[\"attention_mask\"][0],\n",
        "                     },\n",
        "            \"labels\": torch.tensor(label,dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "J6pvsqWPIKRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicDataModule(pl.LightningDataModule):\n",
        "    def __init__(self,train_path,val_path,batch_size=12,max_len=100):\n",
        "        super().__init__()\n",
        "        self.train_path,self.val_path= train_path,val_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def setup(self,stage=None):\n",
        "        train = pd.read_csv(self.train_path)\n",
        "        val = pd.read_csv(self.val_path)\n",
        "        self.train_dataset = ArabicDataset(data=train,max_len=self.max_len)\n",
        "        self.val_dataset = ArabicDataset(data=val,max_len=self.max_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "_A8IYrfYIuJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 3\n",
        "class ArabicBertModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = \"aubmindlab/bert-base-arabertv2\"\n",
        "        self.bert_model = AutoModel.from_pretrained(model)\n",
        "        self.fc = nn.Linear(768, n_classes)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        out = self.bert_model(**inputs)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n",
        "        last_hidden_states = out[1]\n",
        "        out = self.fc(last_hidden_states)\n",
        "        return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.AdamW(self.parameters(), lr=0.0001)\n",
        "\n",
        "    def criterion(self,output,target):\n",
        "        return nn.CrossEntropyLoss()(output,target)\n",
        "\n",
        "    #TODO: adding metrics\n",
        "    def training_step(self,batch,batch_idx):\n",
        "        x,y = batch[\"inputs\"],batch[\"labels\"]\n",
        "        out = self(x)\n",
        "        loss = self.criterion(out,y)\n",
        "        metrics = {\"train_loss\": loss}\n",
        "        self.log_dict(metrics)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self,batch,batch_idx):\n",
        "        x, y = batch[\"inputs\"],batch[\"labels\"]\n",
        "        out = self(x)\n",
        "        loss = self.criterion(out,y)\n",
        "        metrics = {\"val_loss\": loss}\n",
        "        self.log_dict(metrics)\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "k5SEKCq6JMoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dm = ArabicDataModule(train_path=\"train.csv\",\n",
        "                val_path = \"val.csv\",\n",
        "                batch_size=128, max_len=70)\n",
        "\n",
        "model = ArabicBertModel()\n",
        "trainer = pl.Trainer(max_epochs=10, default_root_dir='.') #callbacks=[EarlyStopping(monitor=\"val_f1\")]\n",
        "trainer.fit(model,dm)"
      ],
      "metadata": {
        "id": "EbKIb78VJrYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'arabert_financialnews_sentimentanalysis.pth')"
      ],
      "metadata": {
        "id": "9qPBynsaX16E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load('arabert_financialnews_sentimentanalysis.pth',  map_location=device)\n",
        "model.to(device)\n",
        "\n",
        "preds = []\n",
        "real_values = []\n",
        "\n",
        "test_dataloader = dm.test_dataloader()\n",
        "\n",
        "progress_bar = tqdm(range(len(test_dataloader)))\n",
        "\n",
        "model.eval()\n",
        "for batch in test_dataloader:\n",
        "    x,y = batch[\"inputs\"],batch[\"labels\"]\n",
        "    inp = {k: v.to(device) for k, v in x.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inp)\n",
        "\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    preds.extend(predictions)\n",
        "    real_values.extend(y)\n",
        "\n",
        "    progress_bar.update()\n",
        "\n",
        "preds = torch.stack(preds).cpu()\n",
        "real_values = torch.stack(real_values).cpu()\n",
        "print(classification_report(real_values, preds, target_names=[0, 1, 2]))"
      ],
      "metadata": {
        "id": "LvzugCLsX9qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}